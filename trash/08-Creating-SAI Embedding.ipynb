{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]</td>\n",
       "      <td>Because he's a moron and a bigot. It's not any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[29, 30, 31, 32, 33, 34]</td>\n",
       "      <td>How about we stop protecting idiots and let na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[166, 167, 168, 169, 170, 171]</td>\n",
       "      <td>If people  were  smart, they would  Boycott th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      spans  \\\n",
       "0  [15, 16, 17, 18, 19, 27, 28, 29, 30, 31]   \n",
       "1                  [29, 30, 31, 32, 33, 34]   \n",
       "2            [166, 167, 168, 169, 170, 171]   \n",
       "\n",
       "                                                text  \n",
       "0  Because he's a moron and a bigot. It's not any...  \n",
       "1  How about we stop protecting idiots and let na...  \n",
       "2  If people  were  smart, they would  Boycott th...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spans</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...</td>\n",
       "      <td>Another violent and aggressive immigrant killi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39]</td>\n",
       "      <td>I am 56 years old, I am not your fucking junio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>Damn, a whole family. Sad indeed.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               spans  \\\n",
       "0  [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,...   \n",
       "1                       [33, 34, 35, 36, 37, 38, 39]   \n",
       "2                                       [0, 1, 2, 3]   \n",
       "\n",
       "                                                text  \n",
       "0  Another violent and aggressive immigrant killi...  \n",
       "1  I am 56 years old, I am not your fucking junio...  \n",
       "2                  Damn, a whole family. Sad indeed.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import __init__\n",
    "import models.cross_validator as cross_validator\n",
    "import models.datahandler as datahandler\n",
    "from models.datamodel import DataModel\n",
    "import models.outputmaker as outputmaker\n",
    "import evaluator.metrics as metrics\n",
    "from models.cmm import CMM\n",
    "\n",
    "trial = datahandler.load_train('../data/dataset/tsd_trial.csv', verbose=True)\n",
    "train = datahandler.load_train('../data/dataset/tsd_train.csv', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `Models` Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on train data, test on trial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CMM()\n",
    "\n",
    "datamodel = DataModel(model='cmm', remove_stop_words=True)\n",
    "\n",
    "output_maker = outputmaker.crf_output\n",
    "\n",
    "evaluator = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7939/7939 [02:11<00:00, 60.39it/s]\n",
      "100%|██████████| 690/690 [00:11<00:00, 61.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, train_taboo_words, train_texts = cross_validator.load_data(train, [i for i in range(train.shape[0])],\n",
    "                                                                             datamodel, logger=True)\n",
    "\n",
    "X_test, y_test, test_taboo_words, test_texts = cross_validator.load_data(trial, [i for i in range(trial.shape[0])], \n",
    "                                                                   datamodel, logger=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before doing some preprocessing, only using stopwordremoval\n",
    "* CNS Train: `{'normal': 158417, 'toxic': 18451}`\n",
    "* CNS Test: `{'normal': 13283, 'toxic': 1405}`\n",
    "\n",
    "# After doing some preprocessing and using stopwordremoval\n",
    "* CNS Train: `{'normal': 131143, 'toxic': 17168}`\n",
    "* CNS Test: `{'normal': 11197, 'toxic': 1335}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = []\n",
    "for X in X_train:\n",
    "    for x in X:\n",
    "        train_words.append(x[0])\n",
    "\n",
    "test_words = []\n",
    "for X in X_test:\n",
    "    for x in X:\n",
    "        test_words.append(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train words: 148311\n",
      "size of test words: 12532\n",
      "size of train words after set: 22682\n",
      "size of test words after set: 5088\n"
     ]
    }
   ],
   "source": [
    "print(\"size of train words:\", len(train_words))\n",
    "print(\"size of test words:\", len(test_words))\n",
    "\n",
    "print(\"size of train words after set:\", len(set(train_words)))\n",
    "print(\"size of test words after set:\", len(set(test_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_set = set(train_words)\n",
    "test_word_set = set(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train words after stemming: 13148\n",
      "size of test words after stemming: 3570\n"
     ]
    }
   ],
   "source": [
    "train_word_set_stem = list(set([stemmer.stem(word) for word in train_word_set]))\n",
    "test_word_set_stem = list(set([stemmer.stem(word) for word in test_word_set]))\n",
    "\n",
    "print(\"size of train words after stemming:\", len(set(train_word_set_stem)))\n",
    "print(\"size of test words after stemming:\", len(set(test_word_set_stem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined lenght: 13673\n",
      "['hypocrisi', 'pace', 'preview', 'seth', 'africa', 'kiddi', 'ador', 'heir', 'moan', 'suppos']\n"
     ]
    }
   ],
   "source": [
    "combined_words = list(set([ word for word in train_word_set_stem] + [word for word in test_word_set_stem]))\n",
    "\n",
    "print(\"Combined lenght:\", len(combined_words))\n",
    "\n",
    "print(combined_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sia(word):\n",
    "    return vader.polarity_scores(word)['compound']\n",
    "\n",
    "def get_vader(X):\n",
    "    return [[sia(word)] for word in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sia_arr = np.array(get_vader(combined_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [ 0.    ],\n",
       "        [-0.1531],\n",
       "        [ 0.    ]]),\n",
       " (13673, 1))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia_arr[:10], sia_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10140381286297358, -0.005290777444598845)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(sia_arr), np.mean(sia_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.graph import graph_shortest_path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def distance_mat(X_emo, n_neighbors=6, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute the square distance matrix using Euclidean distance\n",
    "    :param X: Input data, a numpy array of shape (img_height, img_width)\n",
    "    :param n_neighbors: Number of nearest neighbors to consider, int\n",
    "    :return: numpy array of shape (img_height, img_height), numpy array of shape (img_height, n_neighbors)\n",
    "    \"\"\"\n",
    "    def dist1(a, b):\n",
    "        if a == 0 and b == 0:\n",
    "            return 0.0\n",
    "        return np.sqrt(sum((a - b)**2))\n",
    "    \n",
    "    def dist2(a, b):\n",
    "        return np.sqrt(sum((a - b)**2))\n",
    "    if verbose:\n",
    "        print(\"Compute full distance matrix\\n------------------\\n\")\n",
    "        \n",
    "    distances = np.array([[dist1(p1, p2) for p2 in X_emo] for p1 in tqdm(X_emo)])\n",
    "    #distances = np.array([[dist2(p1, p2) for p2 in X] for p1 in tqdm(X)])\n",
    "    #distances = (distances + distances_emo)/2\n",
    "    if verbose:\n",
    "        print(\"DISTANCE.SHAPE:\",distances.shape)\n",
    "        print(\"DISTANCE::\\n------------------\\n\",distances)\n",
    "        print(\"Keep only the {} nearest neighbors, others set to 0 (= unreachable)\".format(n_neighbors))\n",
    "        \n",
    "    neighbors = np.zeros_like(distances)\n",
    "    sort_distances = np.argsort(distances, axis=1)[:, 1:n_neighbors+1] #distances_emo\n",
    "    for k,i in enumerate(sort_distances):\n",
    "        neighbors[k,i] = distances[k,i]\n",
    "    if verbose:\n",
    "        print(\"NEIGHBORS::\\n------------------\\n\",neighbors)\n",
    "        print(\"SORTED  DISTANCES:\\n--------------------\\n\",sort_distances)\n",
    "    return neighbors, sort_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(K):\n",
    "    \"\"\"\n",
    "    Method to center the distance matrix\n",
    "    :param K: numpy array of shape mxm\n",
    "    :return: numpy array of shape mxm\n",
    "    \"\"\"\n",
    "    n_samples = K.shape[0]\n",
    "\n",
    "    # Mean for each row/column\n",
    "    meanrows = np.sum(K, axis=0) / n_samples\n",
    "    meancols = (np.sum(K, axis=1)/n_samples)[:, np.newaxis]\n",
    "\n",
    "    # Mean across all rows (entire matrix)\n",
    "    meanall = meanrows.sum() / n_samples\n",
    "\n",
    "    K -= meanrows\n",
    "    K -= meancols\n",
    "    K += meanall\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mds(data, n_components=2):\n",
    "    \"\"\"\n",
    "    Apply multidimensional scaling (aka Principal Coordinates Analysis)\n",
    "    :param data: nxn square distance matrix\n",
    "    :param n_components: number of components for projection\n",
    "    :return: projected output of shape (n_components, n)\n",
    "    \"\"\"\n",
    "\n",
    "    # Center distance matrix\n",
    "    data = center(data)\n",
    "    \n",
    "    # Make a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_val_cov, eig_vec_cov = np.linalg.eig(data)\n",
    "    eig_pairs = [\n",
    "        (np.abs(eig_val_cov[i]), eig_vec_cov[:, i]) for i in range(len(eig_val_cov))\n",
    "    ]\n",
    "    # Select n_components eigenvectors with largest eigenvalues, obtain subspace transform matrix\n",
    "    eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "    eig_pairs = np.array(eig_pairs)\n",
    "    matrix_w = np.hstack(\n",
    "        [eig_pairs[i, 1].reshape(data.shape[1], 1) for i in range(n_components)]\n",
    "    )\n",
    "\n",
    "    # Return samples in new subspace\n",
    "    return matrix_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isomap(data, n_components=2, n_neighbors=6, verbose=False):\n",
    "    \"\"\"\n",
    "    Dimensionality reduction with isomap algorithm\n",
    "    :param data: input image matrix of shape (n,m) if dist=False, square distance matrix of size (n,n) if dist=True\n",
    "    :param n_components: number of components for projection\n",
    "    :param n_neighbors: number of neighbors for distance matrix computation\n",
    "    :return: Projected output of shape (n_components, n)\n",
    "    \"\"\"\n",
    "    # Compute distance matrix\n",
    "    data, _ = distance_mat(data, n_neighbors, verbose=verbose)\n",
    "\n",
    "    # Compute shortest paths from distance matrix\n",
    "    #from sklearn.utils.graph import graph_shortest_path\n",
    "    graph = graph_shortest_path(data, directed=False)\n",
    "    graph = -0.5 * (graph ** 2)\n",
    "    \n",
    "    # Return the MDS projection on the shortest paths graph\n",
    "    return mds(graph, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words_lsts = train_word_set_stem\n",
    "train_vader_lsts = np.array(get_vader(train_word_set_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 12584/13148 [10:42<00:25, 22.00it/s]"
     ]
    }
   ],
   "source": [
    "isomap = isomap(train_vader_lsts, n_components=30, n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
